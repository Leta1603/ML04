{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c494d8-8fed-4e41-9361-903f57ab0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a5762-dad2-42ad-a8a2-07a2b93f80ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82055d65-ae10-4e60-93cb-6448d1c2f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28d0ab1-d0e7-453d-80f8-7d2525cb4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#левый угол рта — 61, правый угол — 291\n",
    "# верхняя середина губ — 13, нижняя середина — 14\n",
    "\n",
    "LEFT_MOUTH = 61\n",
    "RIGHT_MOUTH = 291\n",
    "UPPER_LIP_CENTER = 13\n",
    "LOWER_LIP_CENTER = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85335f29-c06d-40a7-aada-3ca6bd61b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a, b): return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76858bc-db2b-4436-86f8-3af4df795813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smile_score(landmarks, w, h):\n",
    "    def p(i):\n",
    "        lm = landmarks[i]\n",
    "        return np.array([lm.x * w, lm.y * h], dtype=np.float32)\n",
    "    left, right = p(LEFT_MOUTH), p(RIGHT_MOUTH)\n",
    "    up, low = p(UPPER_LIP_CENTER), p(LOWER_LIP_CENTER)\n",
    "    mid = (up + low) / 2.0\n",
    "\n",
    "    width = euclid(left, right) + 1e-6\n",
    "    height = euclid(up, low) + 1e-6\n",
    "    ratio = width / height\n",
    "    lift = ((mid[1] - left[1]) + (mid[1] - right[1])) / 2.0\n",
    "    return ratio, lift, {\"left\":left, \"right\":right, \"up\":up, \"low\":low, \"mid\":mid}\n",
    "\n",
    "def run_video(source=0, ratio_th=2.15, lift_th=2.0, smooth_window=7):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Не удалось открыть видео/камеру\")\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False, max_num_faces=1, refine_landmarks=False,\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as mesh:\n",
    "        q_ratio = collections.deque(maxlen=smooth_window)\n",
    "        q_lift = collections.deque(maxlen=smooth_window)\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "\n",
    "            h, w = frame.shape[:2]\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = mesh.process(rgb)\n",
    "\n",
    "            label = \"No face\"\n",
    "            if res.multi_face_landmarks:\n",
    "                face = res.multi_face_landmarks[0].landmark\n",
    "                r, l, pts = compute_smile_score(face, w, h)\n",
    "                q_ratio.append(r)\n",
    "                q_lift.append(l)\n",
    "\n",
    "                r_s = float(np.mean(q_ratio))\n",
    "                l_s = float(np.mean(q_lift))\n",
    "                smiling = (r_s > ratio_th) and (l_s > lift_th)\n",
    "                label = f\"{'SMILE' if smiling else 'NEUTRAL'} | r={r_s:.2f}, lift={l_s:.1f}px\"\n",
    "\n",
    "                # Рисуем опорные точки\n",
    "                for key in [\"left\", \"right\", \"up\", \"low\", \"mid\"]:\n",
    "                    x, y = pts[key].astype(int)\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "            cv2.putText(frame, label, (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 220, 0), 2)\n",
    "            cv2.imshow(\"Smile detection (MediaPipe)\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e25f5ca-11c8-45cd-9ded-8f6af9fc59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_video(\"data/video_1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2936a2f-65a0-42f4-8046-0611b2ffee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMesh индексы\n",
    "LEFT_MOUTH, RIGHT_MOUTH = 61, 291\n",
    "UPPER_LIP_CENTER, LOWER_LIP_CENTER = 13, 14\n",
    "LEFT_EYE_OUT, RIGHT_EYE_OUT = 33, 263  # для нормализации\n",
    "\n",
    "def euclid(a, b): \n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "def compute_features(landmarks, w, h):\n",
    "    \"\"\"Возвращает признаки и опорные точки в пикселях + нормализацию на межзрачковое расстояние.\"\"\"\n",
    "    def p(i):\n",
    "        lm = landmarks[i]\n",
    "        return np.array([lm.x * w, lm.y * h], dtype=np.float32)\n",
    "\n",
    "    L, R = p(LEFT_MOUTH), p(RIGHT_MOUTH)\n",
    "    U, D = p(UPPER_LIP_CENTER), p(LOWER_LIP_CENTER)\n",
    "    M = (U + D) / 2.0\n",
    "    EL, ER = p(LEFT_EYE_OUT), p(RIGHT_EYE_OUT)\n",
    "\n",
    "    eye_dist = euclid(EL, ER) + 1e-6        # масштаб лица\n",
    "    mw = euclid(L, R)                        # ширина рта, px\n",
    "    mh = euclid(U, D) + 1e-6                 # высота рта, px\n",
    "    ratio = mw / mh                          # «шире/выше» (безразмерный)\n",
    "    lift_px = ((M[1] - L[1]) + (M[1] - R[1])) / 2.0   # >0 — уголки выше центра (ось Y вниз)\n",
    "    lift_n = lift_px / eye_dist              # нормализованный подъём/опускание уголков\n",
    "    mh_n = mh / eye_dist                     # высота рта в долях расстояния между глазами\n",
    "\n",
    "    return {\n",
    "        \"ratio\": ratio,      # ширина/высота рта\n",
    "        \"lift_n\": lift_n,    # подъём уголков (норм.)\n",
    "        \"mh_n\": mh_n,        # высота рта (норм.)\n",
    "    }, {\"L\":L, \"R\":R, \"U\":U, \"D\":D, \"M\":M}\n",
    "\n",
    "def classify_expression(ratio, lift_n,\n",
    "                        ratio_smile_th=2.20,\n",
    "                        lift_up_th=0.030,\n",
    "                        lift_down_th=0.025):\n",
    "    \"\"\"\n",
    "    Пороги:\n",
    "    - lift_* в долях межзрачкового расстояния (~0.03 ≈ 3% от eye_dist).\n",
    "    - ratio_smile_th: чем выше, тем «строже» определение улыбки.\n",
    "    \"\"\"\n",
    "    if (ratio > ratio_smile_th) and (lift_n > lift_up_th):\n",
    "        return \"SMILE\"\n",
    "    elif (lift_n < -lift_down_th):\n",
    "        return \"SAD\"\n",
    "    else:\n",
    "        return \"NEUTRAL\"\n",
    "\n",
    "def run_video_3states(source=0, smooth_window=7,\n",
    "                      ratio_smile_th=2.20, lift_up_th=0.030, lift_down_th=0.025,\n",
    "                      draw_points=True):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Не удалось открыть видео/камеру\")\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as mesh:\n",
    "        q_ratio = collections.deque(maxlen=smooth_window)\n",
    "        q_lift  = collections.deque(maxlen=smooth_window)\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            h, w = frame.shape[:2]\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = mesh.process(rgb)\n",
    "\n",
    "            label = \"No face\"\n",
    "            if res.multi_face_landmarks:\n",
    "                face = res.multi_face_landmarks[0].landmark\n",
    "                feats, pts = compute_features(face, w, h)\n",
    "\n",
    "                q_ratio.append(feats[\"ratio\"])\n",
    "                q_lift.append(feats[\"lift_n\"])\n",
    "\n",
    "                ratio_s = float(np.mean(q_ratio))\n",
    "                lift_s  = float(np.mean(q_lift))\n",
    "\n",
    "                state = classify_expression(\n",
    "                    ratio_s, lift_s,\n",
    "                    ratio_smile_th=ratio_smile_th,\n",
    "                    lift_up_th=lift_up_th,\n",
    "                    lift_down_th=lift_down_th\n",
    "                )\n",
    "                label = f\"{state} | r={ratio_s:.2f} lift={lift_s:+.3f}\"\n",
    "\n",
    "                if draw_points:\n",
    "                    for key in [\"L\",\"R\",\"U\",\"D\",\"M\"]:\n",
    "                        x, y = pts[key].astype(int)\n",
    "                        cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "                # Цвет рамки/текста по классу\n",
    "                color = {\"SMILE\": (0,200,0), \"SAD\": (0,0,220), \"NEUTRAL\": (200,200,0)}[state]\n",
    "                cv2.rectangle(frame, (10,10), (w-10, 60), (0,0,0), -1)\n",
    "                cv2.putText(frame, label, (15, 45), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
    "\n",
    "            cv2.imshow(\"Smile/Neutral/Sad (MediaPipe)\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2628eb-e1cb-46da-af02-4b7b0d9640ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_video_3states(\"data/video_1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d50ec44-06b0-4c81-8085-4b452dbd0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_video_3states(\"data/video_2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f62b7408-1ba9-40e5-8da8-3fd8f8c795f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_video_3states(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77967d7-1f86-4db5-bbe0-485e5111c7fe",
   "metadata": {},
   "source": [
    "### 2 часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df57ea22-4686-4490-82d8-696e90559183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f97bb3-c123-4ac7-b536-90e63392dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Вспомогательные функции ---\n",
    "\n",
    "def get_distance(p1, p2):\n",
    "    \"\"\"Рассчитывает евклидово расстояние между двумя точками.\"\"\"\n",
    "    return math.sqrt(((p1[0] - p2[0])**2) + ((p1[1] - p2[1])**2))\n",
    "\n",
    "def get_ear(lm_list, eye_indices, image_w, image_h):\n",
    "    \"\"\"\n",
    "    Рассчитывает коэффициент пропорциональности глаза (EAR) по 6 точкам.\n",
    "    \n",
    "    lm_list: Список всех 468 точек лица\n",
    "    eye_indices: Индексы 6 точек для одного глаза\n",
    "    \"\"\"\n",
    "    # \n",
    "    \n",
    "    # Преобразуем нормализованные координаты в пиксели\n",
    "    p1 = (lm_list[eye_indices[0]].x * image_w, lm_list[eye_indices[0]].y * image_h)\n",
    "    p2 = (lm_list[eye_indices[1]].x * image_w, lm_list[eye_indices[1]].y * image_h)\n",
    "    p3 = (lm_list[eye_indices[2]].x * image_w, lm_list[eye_indices[2]].y * image_h)\n",
    "    p4 = (lm_list[eye_indices[3]].x * image_w, lm_list[eye_indices[3]].y * image_h)\n",
    "    p5 = (lm_list[eye_indices[4]].x * image_w, lm_list[eye_indices[4]].y * image_h)\n",
    "    p6 = (lm_list[eye_indices[5]].x * image_w, lm_list[eye_indices[5]].y * image_h)\n",
    "\n",
    "    # Рассчитываем вертикальные расстояния\n",
    "    vert_dist1 = get_distance(p2, p6)\n",
    "    vert_dist2 = get_distance(p3, p5)\n",
    "\n",
    "    # Рассчитываем горизонтальное расстояние\n",
    "    horiz_dist = get_distance(p1, p4)\n",
    "    \n",
    "    if horiz_dist == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Рассчитываем EAR\n",
    "    ear = (vert_dist1 + vert_dist2) / (2.0 * horiz_dist)\n",
    "    return ear\n",
    "\n",
    "# --- Инициализация ---\n",
    "\n",
    "# Индексы MediaPipe для 6 точек вокруг каждого глаза (P1-P6)\n",
    "# P1, P4 - горизонтальные края\n",
    "# P2, P3 - верхнее веко\n",
    "# P5, P6 - нижнее веко\n",
    "\n",
    "# ПРАВЫЙ глаз (с точки зрения человека)\n",
    "RIGHT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "# ЛЕВЫЙ глаз (с точки зрения человека)\n",
    "LEFT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Порог EAR. Если EAR ниже этого значения, считаем глаз закрытым.\n",
    "# Возможно, вам придется его немного подстроить!\n",
    "EYE_EAR_THRESHOLD = 0.2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# --- Основной цикл ---\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Получаем размеры кадра\n",
    "    image_h, image_w, _ = image.shape\n",
    "    \n",
    "    # Конвертируем BGR в RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Обработка MediaPipe\n",
    "    image_rgb.flags.writeable = False\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    image_rgb.flags.writeable = True\n",
    "    \n",
    "    # Обратно в BGR для OpenCV\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    status = \"Eyes Open\"\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        # Получаем точки для первого (и единственного) лица\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        lm = face_landmarks.landmark\n",
    "        \n",
    "        # Рассчитываем EAR для правого глаза\n",
    "        right_ear = get_ear(lm, RIGHT_EYE_INDICES, image_w, image_h)\n",
    "        \n",
    "        # Рассчитываем EAR для левого глаза\n",
    "        left_ear = get_ear(lm, LEFT_EYE_INDICES, image_w, image_h)\n",
    "        \n",
    "        # Если ОБА глаза закрыты (ниже порога)\n",
    "        if right_ear < EYE_EAR_THRESHOLD and left_ear < EYE_EAR_THRESHOLD:\n",
    "            status = \"Eyes Closed\"\n",
    "\n",
    "    # --- Отображение ---\n",
    "    \n",
    "    # Переворачиваем кадр для \"зеркального\" отображения (как в зеркале)\n",
    "    flipped_image = cv2.flip(image_bgr, 1)\n",
    "\n",
    "    # Устанавливаем цвет текста в зависимости от статуса\n",
    "    color = (0, 0, 255) if status == \"Eyes Closed\" else (0, 255, 0)\n",
    "    \n",
    "    # Рисуем статус на перевернутом кадре\n",
    "    cv2.putText(flipped_image, status, \n",
    "                (30, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Показываем результат\n",
    "    cv2.imshow('MediaPipe Eye Detector', flipped_image)\n",
    "\n",
    "    # Выход по нажатию 'ESC'\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Освобождаем ресурсы\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "face_mesh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634877c-661f-4d5f-b562-8257500a013a",
   "metadata": {},
   "source": [
    "### 3 Тело и подъем рук с хлопком над головой счетчик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "388cdc98-5987-4a18-9336-d274271958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "PL = mp_pose.PoseLandmark\n",
    "\n",
    "# Вспомогательные функции\n",
    "def to_np(lm, w, h):\n",
    "    return np.array([lm.x * w, lm.y * h], dtype=np.float32)\n",
    "\n",
    "def euclid(a, b):\n",
    "    return float(np.linalg.norm(a - b))\n",
    "\n",
    "def run_pose_clap_counter(\n",
    "    source=0,\n",
    "    smooth_window=5,          # сглаживание расстояния между кистями\n",
    "    clap_thresh_rel=0.35,     # порог «близко»: доля ширины плеч\n",
    "    min_visibility=0.5,       # минимальная видимость точки\n",
    "    min_frames_between=6,     # дебаунс: минимум кадров между хлопками\n",
    "    draw_skeleton=True\n",
    "):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Не удалось открыть видео/камеру\")\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        smooth_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as pose:\n",
    "\n",
    "        clap_count = 0\n",
    "        was_apart = True          # предыдущая фаза (кисти были далеко)\n",
    "        frames_since_last = 999   # дебаунс-счётчик\n",
    "        q_dist = collections.deque(maxlen=smooth_window)\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            h, w = frame.shape[:2]\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = pose.process(rgb)\n",
    "\n",
    "            label = \"No body\"\n",
    "            arms_up = False\n",
    "\n",
    "            if res.pose_landmarks:\n",
    "                lms = res.pose_landmarks.landmark\n",
    "                # Достаём нужные точки\n",
    "                pts = {}\n",
    "                for name in [\n",
    "                    PL.LEFT_WRIST, PL.RIGHT_WRIST,\n",
    "                    PL.LEFT_SHOULDER, PL.RIGHT_SHOULDER,\n",
    "                    PL.LEFT_EYE, PL.RIGHT_EYE\n",
    "                ]:\n",
    "                    lm = lms[name.value]\n",
    "                    pts[name] = (to_np(lm, w, h), lm.visibility)\n",
    "\n",
    "                # Проверяем видимость ключевых точек\n",
    "                needed = [PL.LEFT_WRIST, PL.RIGHT_WRIST, PL.LEFT_SHOULDER, PL.RIGHT_SHOULDER, PL.LEFT_EYE, PL.RIGHT_EYE]\n",
    "                if all(pts[p][1] >= min_visibility for p in needed):\n",
    "                    LW, RW = pts[PL.LEFT_WRIST][0], pts[PL.RIGHT_WRIST][0]\n",
    "                    LS, RS = pts[PL.LEFT_SHOULDER][0], pts[PL.RIGHT_SHOULDER][0]\n",
    "                    LE, RE = pts[PL.LEFT_EYE][0], pts[PL.RIGHT_EYE][0]\n",
    "\n",
    "                    # Геометрия (в пикселях)\n",
    "                    shoulder_width = euclid(LS, RS) + 1e-6\n",
    "                    wrist_dist = euclid(LW, RW)\n",
    "\n",
    "                    # Нормированное сближение кистей\n",
    "                    wrist_dist_rel = wrist_dist / shoulder_width\n",
    "                    q_dist.append(wrist_dist_rel)\n",
    "                    wrist_dist_smooth = float(np.mean(q_dist))\n",
    "\n",
    "                    # Руки подняты: обе кисти выше уровня глаз\n",
    "                    eye_y = min(LE[1], RE[1])\n",
    "                    arms_up = (LW[1] < eye_y) and (RW[1] < eye_y)\n",
    "\n",
    "                    # Хлопок: кисти близко И руки подняты\n",
    "                    close_enough = wrist_dist_smooth < clap_thresh_rel\n",
    "                    frames_since_last += 1\n",
    "\n",
    "                    if arms_up and close_enough and was_apart and frames_since_last >= min_frames_between:\n",
    "                        clap_count += 1\n",
    "                        was_apart = False\n",
    "                        frames_since_last = 0\n",
    "                    elif not close_enough:\n",
    "                        was_apart = True\n",
    "\n",
    "                    # Текст состояния\n",
    "                    state = []\n",
    "                    state.append(\"ARMS_UP\" if arms_up else \"ARMS_DOWN\")\n",
    "                    state.append(\"CLOSE\" if close_enough else \"APART\")\n",
    "                    label = f\"{' | '.join(state)}  dist={wrist_dist_smooth:.2f}  cnt={clap_count}\"\n",
    "\n",
    "                    # Рисуем направляющие\n",
    "                    if draw_skeleton:\n",
    "                        mp.solutions.drawing_utils.draw_landmarks(\n",
    "                            frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp.solutions.drawing_styles.get_default_pose_landmarks_style()\n",
    "                        )\n",
    "                        # Линия между запястьями\n",
    "                        cv2.line(frame, tuple(LW.astype(int)), tuple(RW.astype(int)), (0, 255, 255), 2)\n",
    "                        # Уровень глаз\n",
    "                        cv2.line(frame, (0, int(eye_y)), (w, int(eye_y)), (128, 128, 128), 1, cv2.LINE_AA)\n",
    "\n",
    "            # HUD\n",
    "            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "            cv2.rectangle(frame, (10, 10), (20 + tw, 20 + th), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, label, (15, 15 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 200), 2)\n",
    "            cv2.rectangle(frame, (w - 160, 10), (w - 10, 70), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, f\"CLAPS: {clap_count}\", (w - 150, 55), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 220, 255), 2)\n",
    "\n",
    "            cv2.imshow(\"Pose Clap Counter\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b55651e8-471f-46aa-8e02-4e4cf1dc5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pose_clap_counter(source=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "247369d3-5f00-46f2-9c5b-718f68b552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "PL = mp_pose.PoseLandmark\n",
    "\n",
    "def to_np(lm, w, h): return np.array([lm.x * w, lm.y * h], dtype=np.float32)\n",
    "def euclid(a, b): return float(np.linalg.norm(a - b))\n",
    "\n",
    "def run_pose_clap_counter_jump(\n",
    "    source=0,\n",
    "    # --- хлопок (как раньше, гистерезис) ---\n",
    "    smooth_window=5,\n",
    "    close_th_rel=0.38,\n",
    "    open_th_rel=0.60,\n",
    "    min_visibility=0.5,\n",
    "    min_frames_between=6,\n",
    "    require_above_head=True,\n",
    "    use_index_tips=True,\n",
    "    # --- прыжок ---\n",
    "    up_start_delta_rel=0.10,      # насколько таз поднялся от \"земли\", чтобы считать прыжок начатым (доля ширины плеч)\n",
    "    up_vel_th_rel=0.015,          # минимальная \"скорость\" вверх (на кадр, в долях ширины плеч)\n",
    "    ground_return_delta_rel=0.03, # считаем, что вернулись на землю, когда подъём < этого порога\n",
    "    one_clap_per_jump=True,       # только 1 хлопок за прыжок\n",
    "    draw_skeleton=True\n",
    "):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Не удалось открыть видео/камеру\")\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False, model_complexity=1,\n",
    "        enable_segmentation=False, smooth_landmarks=True,\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as pose:\n",
    "\n",
    "        clap_count = 0\n",
    "        frames_since_last = 999\n",
    "\n",
    "        # сглаживание расстояния между руками\n",
    "        q_dist = collections.deque(maxlen=smooth_window)\n",
    "        prev_smooth = None\n",
    "        state_close = False\n",
    "\n",
    "        # прыжок: отслеживаем вертикаль таза\n",
    "        prev_hip_y = None\n",
    "        hip_ground_ema = None       # \"уровень земли\" (эксп. среднее)\n",
    "        ema_alpha = 0.02            # скорость обновления уровня земли\n",
    "\n",
    "        jump_state = \"GROUND\"       # GROUND | ASCEND | AIRBORNE | DESCEND\n",
    "        clap_in_this_jump = False\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            h, w = frame.shape[:2]\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = pose.process(rgb)\n",
    "\n",
    "            hud = \"No body\"\n",
    "            if res.pose_landmarks:\n",
    "                lms = res.pose_landmarks.landmark\n",
    "\n",
    "                names = [\n",
    "                    PL.LEFT_SHOULDER, PL.RIGHT_SHOULDER,\n",
    "                    PL.LEFT_EYE, PL.RIGHT_EYE,\n",
    "                    PL.LEFT_HIP, PL.RIGHT_HIP,\n",
    "                    PL.LEFT_WRIST, PL.RIGHT_WRIST,\n",
    "                    PL.LEFT_INDEX, PL.RIGHT_INDEX\n",
    "                ]\n",
    "                pts = {n: (to_np(lms[n.value], w, h), lms[n.value].visibility) for n in names}\n",
    "\n",
    "                need = [PL.LEFT_SHOULDER, PL.RIGHT_SHOULDER, PL.LEFT_EYE, PL.RIGHT_EYE, PL.LEFT_HIP, PL.RIGHT_HIP]\n",
    "                if all(pts[n][1] >= min_visibility for n in need):\n",
    "\n",
    "                    # геометрия\n",
    "                    LS, RS = pts[PL.LEFT_SHOULDER][0], pts[PL.RIGHT_SHOULDER][0]\n",
    "                    shoulder_width = euclid(LS, RS) + 1e-6\n",
    "                    mid_shoulders_x = (LS[0] + RS[0]) / 2.0\n",
    "\n",
    "                    hipL, hipR = pts[PL.LEFT_HIP][0], pts[PL.RIGHT_HIP][0]\n",
    "                    hip_y = float((hipL[1] + hipR[1]) / 2.0)   # ниже = больше; вверх = меньше\n",
    "\n",
    "                    # уровень \"земли\" (обновляем, пока мы реально на земле)\n",
    "                    if hip_ground_ema is None:\n",
    "                        hip_ground_ema = hip_y\n",
    "                    # критерий \"похоже стоим\": небольшая вертикальная скорость и руки не над головой\n",
    "                    # (чтобы в верхней фазе прыжка не портить уровень земли)\n",
    "                    if prev_hip_y is not None:\n",
    "                        vel_y = hip_y - prev_hip_y              # <0 вверх, >0 вниз\n",
    "                    else:\n",
    "                        vel_y = 0.0\n",
    "                    prev_hip_y = hip_y\n",
    "\n",
    "                    # нормализованные величины прыжка\n",
    "                    # чем больше delta_up_rel, тем выше относительно земли мы поднялись\n",
    "                    delta_up_rel = (hip_ground_ema - hip_y) / shoulder_width\n",
    "                    vel_up_rel = (-vel_y) / shoulder_width     # >0 движемся вверх\n",
    "\n",
    "                    # обновление уровня земли (если явно не прыгаем)\n",
    "                    if abs(vel_up_rel) < 0.005 and delta_up_rel < 0.02:\n",
    "                        hip_ground_ema = (1 - ema_alpha) * hip_ground_ema + ema_alpha * hip_y\n",
    "\n",
    "                    # детектор прыжка (простая машина состояний)\n",
    "                    if jump_state == \"GROUND\":\n",
    "                        if delta_up_rel > up_start_delta_rel or vel_up_rel > up_vel_th_rel:\n",
    "                            jump_state = \"ASCEND\"\n",
    "                            clap_in_this_jump = False\n",
    "                    elif jump_state == \"ASCEND\":\n",
    "                        if vel_up_rel <= 0:          # достигли вершины\n",
    "                            jump_state = \"AIRBORNE\"\n",
    "                    elif jump_state == \"AIRBORNE\":\n",
    "                        if vel_up_rel < -0.005:      # уверенно пошли вниз\n",
    "                            jump_state = \"DESCEND\"\n",
    "                    elif jump_state == \"DESCEND\":\n",
    "                        if delta_up_rel < ground_return_delta_rel:\n",
    "                            jump_state = \"GROUND\"\n",
    "\n",
    "                    # точки рук: указательные или запястья\n",
    "                    use_index = use_index_tips and pts[PL.LEFT_INDEX][1] >= min_visibility and pts[PL.RIGHT_INDEX][1] >= min_visibility\n",
    "                    Lp = pts[PL.LEFT_INDEX][0] if use_index else pts[PL.LEFT_WRIST][0]\n",
    "                    Rp = pts[PL.RIGHT_INDEX][0] if use_index else pts[PL.RIGHT_WRIST][0]\n",
    "\n",
    "                    # расстояние между руками (норм.)\n",
    "                    raw_dist = euclid(Lp, Rp)\n",
    "                    dist_rel = raw_dist / shoulder_width\n",
    "                    q_dist.append(dist_rel)\n",
    "                    smooth = float(np.mean(q_dist))\n",
    "                    deriv = 0.0 if prev_smooth is None else (smooth - prev_smooth)  # >0 расходятся\n",
    "                    prev_smooth = smooth\n",
    "\n",
    "                    # руки подняты и над головой?\n",
    "                    eye_y = min(pts[PL.LEFT_EYE][0][1], pts[PL.RIGHT_EYE][0][1])\n",
    "                    arms_up = (Lp[1] < eye_y) and (Rp[1] < eye_y)\n",
    "\n",
    "                    above_ok = True\n",
    "                    if require_above_head:\n",
    "                        margin = 0.25 * shoulder_width\n",
    "                        head_top_y = eye_y - 0.25 * shoulder_width\n",
    "                        center_ok = (abs(((Lp[0]+Rp[0])/2.0) - mid_shoulders_x) < 0.6 * shoulder_width)\n",
    "                        above_ok = (Lp[1] < head_top_y and Rp[1] < head_top_y and center_ok)\n",
    "\n",
    "                    frames_since_last += 1\n",
    "                    jump_active = jump_state in (\"ASCEND\", \"AIRBORNE\", \"DESCEND\")\n",
    "\n",
    "                    # --- гистерезис для хлопка ---\n",
    "                    if arms_up and above_ok and smooth < close_th_rel and jump_active:\n",
    "                        state_close = True\n",
    "\n",
    "                    ready_to_count = (\n",
    "                        state_close and               # были \"очень близко\"\n",
    "                        deriv > 0 and                 # минимум расстояния пройден\n",
    "                        smooth > open_th_rel and      # разошлись достаточно\n",
    "                        frames_since_last >= min_frames_between and\n",
    "                        arms_up and above_ok and\n",
    "                        jump_active and\n",
    "                        (not one_clap_per_jump or not clap_in_this_jump)\n",
    "                    )\n",
    "                    if ready_to_count:\n",
    "                        clap_count += 1\n",
    "                        frames_since_last = 0\n",
    "                        state_close = False\n",
    "                        clap_in_this_jump = True\n",
    "\n",
    "                    # HUD\n",
    "                    state_txt = []\n",
    "                    state_txt.append(f\"JUMP:{jump_state}\")\n",
    "                    state_txt.append(\"ARMS_UP\" if arms_up else \"ARMS_DOWN\")\n",
    "                    state_txt.append(\"ABOVE_OK\" if above_ok else \"LOW/ASIDE\")\n",
    "                    state_txt.append(\"CLOSE\" if state_close else \"OPEN\")\n",
    "                    hud = f\"{' | '.join(state_txt)}  d={smooth:.2f} d'={deriv:+.3f}  up={delta_up_rel:+.3f} v_up={vel_up_rel:+.3f}  cnt={clap_count}\"\n",
    "\n",
    "                    if draw_skeleton:\n",
    "                        mp.solutions.drawing_utils.draw_landmarks(\n",
    "                            frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp.solutions.drawing_styles.get_default_pose_landmarks_style()\n",
    "                        )\n",
    "                        # рука-рука\n",
    "                        cv2.line(frame, tuple(Lp.astype(int)), tuple(Rp.astype(int)), (0,255,255), 2)\n",
    "                        # линия глаз\n",
    "                        cv2.line(frame, (0, int(eye_y)), (w, int(eye_y)), (128,128,128), 1, cv2.LINE_AA)\n",
    "                        # уровень \"земли\" для таза (визуально)\n",
    "                        gy = int(hip_ground_ema)\n",
    "                        cv2.line(frame, (0, gy), (w, gy), (80,80,80), 1, cv2.LINE_AA)\n",
    "\n",
    "            # отрисовка HUD\n",
    "            (tw, th), _ = cv2.getTextSize(hud, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            # cv2.rectangle(frame, (10, 10), (20 + tw, 20 + th), (0, 0, 0), -1)\n",
    "            # cv2.putText(frame, hud, (15, 15 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 200), 2)\n",
    "            cv2.rectangle(frame, (w - 160, 10), (w - 10, 70), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, f\"CLAPS: {clap_count}\", (w - 150, 55), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 220, 255), 2)\n",
    "\n",
    "            cv2.namedWindow(\"Pose Clap Counter (jump)\", cv2.WINDOW_NORMAL)\n",
    "            cv2.resizeWindow(\"Pose Clap Counter (jump)\", 1280, 960)\n",
    "\n",
    "            cv2.imshow(\"Pose Clap Counter (jump)\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f38314a-2c18-4aa9-9a3e-b1a5f5dd78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pose_clap_counter_jump(source=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401d23b-0781-460b-9901-ed19a07091c3",
   "metadata": {},
   "source": [
    "## MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7a6e2d-27bf-4b1e-86a9-200c5d95560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26da32f9-4dda-44c4-9f19-b13d6965f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc59dc86-7065-4398-8dd4-ae00cfcc8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_mouth(face):\n",
    "    box = face[\"box\"]  # [x, y, w, h]\n",
    "    keypoints = face[\"keypoints\"]\n",
    "\n",
    "    if \"mouth_left\" not in keypoints or \"mouth_right\" not in keypoints:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    (ml_x, ml_y) = keypoints[\"mouth_left\"]\n",
    "    (mr_x, mr_y) = keypoints[\"mouth_right\"]\n",
    "\n",
    "    face_width = box[2]\n",
    "    if face_width <= 0:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    mouth_width = math.hypot(mr_x - ml_x, mr_y - ml_y)\n",
    "    # Нормализуем по ширине лица (чтобы не зависеть от размера в кадре)\n",
    "    ratio = mouth_width / float(face_width)\n",
    "\n",
    "    # Пороговые значения ПРИМЕРНЫЕ, подбираются под себя\n",
    "    if ratio > 0.5:\n",
    "        return \"Happy\"\n",
    "    elif ratio < 0.35:\n",
    "        return \"Sad\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7df1e1-51cc-4d09-a53f-cc34141470d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_contrast(gray_frame, center, face_width):\n",
    "    \"\"\"\n",
    "    Берём небольшой квадратик вокруг глаза, считаем std (контраст).\n",
    "    Открытый глаз (белок + зрачок) обычно более контрастный, чем закрытый.\n",
    "    \"\"\"\n",
    "    ex, ey = center\n",
    "    h, w = gray_frame.shape[:2]\n",
    "\n",
    "    # Размер окна вокруг глаза ~ 15% ширины лица\n",
    "    size = int(face_width * 0.15)\n",
    "    if size < 4:\n",
    "        return 0.0\n",
    "    half = size // 2\n",
    "\n",
    "    x1 = max(ex - half, 0)\n",
    "    x2 = min(ex + half, w - 1)\n",
    "    y1 = max(ey - half, 0)\n",
    "    y2 = min(ey + half, h - 1)\n",
    "\n",
    "    roi = gray_frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.std(roi))\n",
    "\n",
    "def classify_eyes_state(frame, face):\n",
    "    \"\"\"\n",
    "    Глаза открыты или закрыты — по контрасту в области глаз.\n",
    "    \"\"\"\n",
    "    box = face[\"box\"]  # [x, y, w, h]\n",
    "    keypoints = face[\"keypoints\"]\n",
    "    face_width = box[2]\n",
    "\n",
    "    if \"left_eye\" not in keypoints or \"right_eye\" not in keypoints:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    (le_x, le_y) = keypoints[\"left_eye\"]\n",
    "    (re_x, re_y) = keypoints[\"right_eye\"]\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    c_left = eye_contrast(gray, (le_x, le_y), face_width)\n",
    "    c_right = eye_contrast(gray, (re_x, re_y), face_width)\n",
    "    avg_contrast = (c_left + c_right) / 2.0\n",
    "\n",
    "    if avg_contrast > 18:  # можно поиграть значением 15–25\n",
    "        return \"Eyes opened\"\n",
    "    else:\n",
    "        return \"Eyes closed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "444cd822-5688-48ce-99b4-35ed69dae3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Для удобства делаем \"селфи\"-зеркало\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # MTCNN работает в RGB\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = detector.detect_faces(rgb)\n",
    "\n",
    "    if faces:\n",
    "        # Берём лицо с максимальной уверенностью\n",
    "        face = max(faces, key=lambda f: f.get(\"confidence\", 0))\n",
    "\n",
    "        box = face[\"box\"]  # [x, y, w, h]\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Нарисуем рамку лица\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # 1) Классификация рта\n",
    "        mouth_state = classify_mouth(face)\n",
    "        mouth_state_text = f\"Mouth: {mouth_state}\"\n",
    "\n",
    "        # 2) Классификация глаз\n",
    "        eyes_state = classify_eyes_state(frame, face)\n",
    "        eyes_state_text = f\"Eyes: {eyes_state}\"\n",
    "\n",
    "        # Нарисуем ключевые точки (глаза, рот, нос)\n",
    "        for name, (kx, ky) in face[\"keypoints\"].items():\n",
    "            cv2.circle(frame, (kx, ky), 3, (255, 0, 0), -1)\n",
    "            cv2.putText(frame, name, (kx + 3, ky - 3),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "\n",
    "    # Выводим текст на экран\n",
    "    cv2.putText(frame, mouth_state_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.putText(frame, eyes_state_text, (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"MTCNN\", frame)\n",
    "\n",
    "    # Выход по 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c0a5b-7894-4987-ae05-3c9946b80fe7",
   "metadata": {},
   "source": [
    "## OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b17be26-1eba-4afb-9161-aa302782b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "smile_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_smile.xml\"\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, scaleFactor=1.3, minNeighbors=5, minSize=(80, 80)\n",
    "    )\n",
    "\n",
    "    status_text = \"The face was not found\"\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Берём нижнюю треть лица как область рта\n",
    "        mouth_y1 = y + int(h * 2 / 3)\n",
    "        mouth_y2 = y + h\n",
    "        mouth_roi_gray = gray[mouth_y1:mouth_y2, x:x + w]\n",
    "\n",
    "        # Пытаемся найти улыбку каскадом\n",
    "        smiles = smile_cascade.detectMultiScale(\n",
    "            mouth_roi_gray,\n",
    "            scaleFactor=1.4,\n",
    "            minNeighbors=20,\n",
    "            minSize=(25, 25)\n",
    "        )\n",
    "\n",
    "        if len(smiles) > 0:\n",
    "            status_text = \"Happy\"\n",
    "        else:\n",
    "            # Если нет явной улыбки, смотрим среднюю яркость нижней части лица\n",
    "            if mouth_roi_gray.size > 0:\n",
    "                mean_intensity = np.mean(mouth_roi_gray)\n",
    "            else:\n",
    "                mean_intensity = 128\n",
    "\n",
    "            if mean_intensity < 80:\n",
    "                status_text = \"Sad\"\n",
    "            else:\n",
    "                status_text = \"Neutral\"\n",
    "\n",
    "        break  # берём только первое лицо\n",
    "\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        status_text,\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.8,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"OpenCV\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca9ca6f-01cd-4fae-b260-e66c3e40b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "eye_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_eye_tree_eyeglasses.xml\"\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, scaleFactor=1.3, minNeighbors=5, minSize=(80, 80)\n",
    "    )\n",
    "\n",
    "    eyes_text = \"\"\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Берём верхнюю половину лица как область глаз\n",
    "        eyes_roi_gray = gray[y:y + int(h / 2), x:x + w]\n",
    "        eyes_roi_color = frame[y:y + int(h / 2), x:x + w]\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(\n",
    "            eyes_roi_gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(20, 20)\n",
    "        )\n",
    "\n",
    "        # Рисуем найденные глаза\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(\n",
    "                eyes_roi_color, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2\n",
    "            )\n",
    "\n",
    "        if len(eyes) >= 2:\n",
    "            eyes_text = \"Eyes: opened\"\n",
    "        else:\n",
    "            eyes_text = \"Eyes: closed\"\n",
    "\n",
    "        break\n",
    "\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        eyes_text,\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.8,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"OpenCV\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b254e-eb82-4ac3-8868-aa3a9f2271a0",
   "metadata": {},
   "source": [
    "# Сравнение MediaPipe, OpenCV и MTCNN для задач курса\n",
    "\n",
    "## Задачи\n",
    "\n",
    "1. Определение по губам: **улыбка / грусть / нейтрально**  \n",
    "2. Определение: **глаза открыты / закрыты**  \n",
    "3. Подсчёт **хлопков в прыжке** (над головой, в реальном времени)\n",
    "\n",
    "---\n",
    "\n",
    "## Сводная таблица\n",
    "\n",
    "| Задача / Библиотека | MediaPipe (Face / Pose) | OpenCV (Haar / классика) | MTCNN |\n",
    "|---------------------|-------------------------|---------------------------|--------|\n",
    "| **1. Улыбка / грусть / нейтрально по рту** | **Лучше всего**: Face Mesh даёт много точных точек губ, можно построить стабильные геометрические признаки | Много ложных срабатываний, сильно зависит от освещения и ракурса | Есть только 2 уголка рта; можно делать простые правила (ширина рта / угол), но сложно отделять тонкие эмоции |\n",
    "| **2. Глаза открыты / закрыты** | **Лучше всего**: Face/Eye Mesh даёт много точек вокруг века, легко устойчиво ловить моргания и закрытые глаза | Очень чувствительно к очкам, свету и поворотам головы | По центрам глаз вырезаются патчи и анализируются контраст/границы; работает, но сильно зависит от условий съёмки |\n",
    "| **3. Хлопки в прыжке (над головой)** | **Однозначный выбор**: Pose даёт скелет (кисти, локти, плечи, бёдра, колени) | Практически непригоден для этой задачи без доп. моделей: классические методы OpenCV не дают скелет и плохо отслеживают руки/прыжки | Вообще не подходит: MTCNN видит только лицо и не даёт никакой информации о руках или теле |\n",
    "\n",
    "---\n",
    "\n",
    "## Плюсы и минусы\n",
    "\n",
    "### MediaPipe\n",
    "\n",
    "**Плюсы:**\n",
    "- Есть модули под разные части тела: Pose, Face Mesh, Hands, Holistic.\n",
    "- Хорошо подходит для:\n",
    "  - позы/жестов (хлопки, прыжки, осанка),\n",
    "  - анализа лица по геометрии (улыбка, глаза, мимика).\n",
    "---\n",
    "\n",
    "### OpenCV (Haar / классические методы)\n",
    "\n",
    "**Плюсы:**\n",
    "- Очень простой старт, минимум зависимостей.\n",
    "- Подходит для демонстрации классического CV:\n",
    "  - поиск лица,\n",
    "  - попытка найти глаза и улыбку каскадами.\n",
    "\n",
    "**Минусы:**\n",
    "- Сильно зависит от освещения, поворота головы, очков.\n",
    "- Для тела/жестов практически бесполезен без сторонних моделей.\n",
    "- Для эмоций и состояния глаз выдаёт много ошибок, всё строится на грубых эвристиках.\n",
    "\n",
    "---\n",
    "\n",
    "### MTCNN\n",
    "\n",
    "**Плюсы:**\n",
    "- Хороший детектор лица с 5 ключевыми точками:\n",
    "  - глаза, нос, уголки рта.\n",
    "- Стабильнее и точнее, чем Haar-каскады, на разных масштабах и при сложном освещении.\n",
    "- Удобен как первый шаг в пайплайне **распознавания лиц**.\n",
    "\n",
    "**Минусы:**\n",
    "- Работает только с **лицом**, тело/руки не видит вообще.\n",
    "- Для эмоций и состояния глаз даёт мало информации (только несколько точек).\n",
    "\n",
    "---\n",
    "\n",
    "## Краткий вывод\n",
    "\n",
    "- Для **хлопков в прыжке** — практически без альтернатив:  **MediaPipe Pose**.\n",
    "- Для **улыбка / грусть / нейтрально** и **глаза открыты / закрыты**:\n",
    "  - учебные, «простые» решения -> можно показать варианты на **OpenCV** и **MTCNN**;\n",
    "  - более качественное и устойчивое решение -> **MediaPipe Face Mesh / Holistic**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af678c3c-ecab-448a-b546-6b4a20d61207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
