{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32502c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Wav2Vec2Model\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f163e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, data_dir, target_sr=16000):\n",
    "        self.data_dir = data_dir\n",
    "        self.target_sr = target_sr\n",
    "\n",
    "        self.classes = sorted(\n",
    "            d.name for d in os.scandir(data_dir) if d.is_dir()\n",
    "        )\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        self.files = []\n",
    "        for cls in self.classes:\n",
    "            pattern = os.path.join(data_dir, cls, \"*.wav\")\n",
    "            for path in glob(pattern):\n",
    "                self.files.append((path, self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.files[idx]\n",
    "\n",
    "        y, sr = librosa.load(path, sr=self.target_sr, mono=True)\n",
    "        waveform = torch.from_numpy(y)\n",
    "\n",
    "        return {'input_values': waveform, 'labels': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2c56f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ashes\\other\\courses_ml\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b6a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [b[\"input_values\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    input_values_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "\n",
    "    return {\"input_values\": input_values_padded, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59da329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(wav2vec, input_values):\n",
    "    with torch.no_grad():\n",
    "        out = wav2vec(input_values=input_values, output_hidden_states=False)\n",
    "    feats = out.last_hidden_state.mean(dim=1)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2e1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2Classifier(nn.Module):\n",
    "    def __init__(self, backbone_name=\"facebook/wav2vec2-base\", num_classes=35):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(backbone_name)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_values, freeze_backbone=True):\n",
    "        if freeze_backbone:\n",
    "            with torch.no_grad():\n",
    "                out = self.backbone(input_values=input_values)\n",
    "        else:\n",
    "            out = self.backbone(input_values=input_values)\n",
    "        x = out.last_hidden_state.mean(dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16bbe92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/speech_commands/'\n",
    "\n",
    "num_classes = len(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb2b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Classifier(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe7bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_split = 'datasets/speech_commands_split/'\n",
    "\n",
    "train_ds = SpeechCommandsDataset(os.path.join(data_dir_split, 'train'))\n",
    "val_ds = SpeechCommandsDataset(os.path.join(data_dir_split, 'val'))\n",
    "test_ds = SpeechCommandsDataset(os.path.join(data_dir_split, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ef49e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def subset_per_class(dataset, max_per_class=1000, seed=42):\n",
    "    random.seed(seed)\n",
    "    by_class = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset.files):\n",
    "        by_class[label].append(idx)\n",
    "\n",
    "    keep_indices = []\n",
    "    for label, idxs in by_class.items():\n",
    "        if len(idxs) > max_per_class:\n",
    "            idxs = random.sample(idxs, max_per_class)\n",
    "        keep_indices.extend(idxs)\n",
    "\n",
    "    subset = torch.utils.data.Subset(dataset, keep_indices)\n",
    "    return subset\n",
    "\n",
    "train_ds = subset_per_class(train_ds, max_per_class=700)\n",
    "val_ds = subset_per_class(val_ds, max_per_class=150)\n",
    "test_ds = subset_per_class(test_ds, max_per_class=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b3805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e128d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, freeze_backbone):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Train:'):\n",
    "        inputs = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(inputs, freeze_backbone=freeze_backbone)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate(model, loader, freeze_backbone=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Val:'):\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            logits = model(inputs, freeze_backbone=freeze_backbone)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a54771c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:: 100%|██████████| 307/307 [00:24<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_loss: 1.4992 acc: 68.9388%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:: 100%|██████████| 66/66 [00:13<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    val_loss: 1.5724 acc: 55.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, freeze_backbone=True)\n",
    "    print(f'    train_loss: {train_loss:.4f} acc: {train_acc:.4%}')\n",
    "    val_loss, val_acc = validate(model, val_loader, freeze_backbone=True)\n",
    "    print(f'    val_loss: {val_loss:.4f} acc: {val_acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "648ad916",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e945c158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:: 100%|██████████| 307/307 [01:24<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_loss: 0.4252 acc: 86.8776%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:: 100%|██████████| 66/66 [00:05<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    val_loss: 0.1029 acc: 96.76%\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:: 100%|██████████| 307/307 [10:54<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_loss: 0.2893 acc: 90.4082%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:: 100%|██████████| 66/66 [00:26<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    val_loss: 0.1686 acc: 94.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, freeze_backbone=False)\n",
    "    print(f'    train_loss: {train_loss:.4f} acc: {train_acc:.4%}')\n",
    "    val_loss, val_acc = validate(model, val_loader, freeze_backbone=False)\n",
    "    print(f'    val_loss: {val_loss:.4f} acc: {val_acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bcb629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:: 100%|██████████| 66/66 [00:33<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Val:'):\n",
    "        inputs = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        logits = model(inputs, freeze_backbone=False)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b253047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1688, acc: 0.9457\n"
     ]
    }
   ],
   "source": [
    "print(f'loss: {avg_loss:.4f}, acc: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45176143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c19d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       150\n",
      "           1       0.86      0.99      0.92       150\n",
      "           2       0.98      0.87      0.93       150\n",
      "           3       0.97      0.92      0.94       150\n",
      "           4       0.98      0.91      0.94       150\n",
      "           5       0.98      0.98      0.98       150\n",
      "           6       0.92      0.96      0.94       150\n",
      "\n",
      "    accuracy                           0.95      1050\n",
      "   macro avg       0.95      0.95      0.95      1050\n",
      "weighted avg       0.95      0.95      0.95      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
