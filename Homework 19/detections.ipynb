{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5676dc4b-bfab-48b1-b807-a03e88a5e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from torchvision.models.detection import ssd300_vgg16, SSD300_VGG16_Weights, fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfd2aed-7399-455c-9e18-a93514edccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANGEABLE_CONDITIONS_VIDEO_PATH = Path(\"data/videos/changeable_conditions.mp4\")\n",
    "CLEAR_WHEATHER_VIDEO_PATH = Path(\"data/videos/clear_weather.mp4\")\n",
    "DARK_VIDEO_PATH = Path(\"data/videos/dark_video.mp4\")\n",
    "POOR_WEATHER_VIDEO_PATH = Path(\"data/videos/poor_weather.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fc5a1a-5504-4376-88d0-864378be0124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Порог для уверенности и IoU (общие для всех)\n",
    "CONF_TH = 0.3\n",
    "IOU_TH = 0.45\n",
    "\n",
    "# Классы \"person\" для разных моделей (COCO)\n",
    "YOLO_PERSON_ID = 0      # COCO: 0 — person\n",
    "SSD_PERSON_LABEL = 1    # COCO: 1 — person\n",
    "FASTER_PERSON_LABEL = 1 # COCO: 1 — person\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24b93418-e63a-4966-8e12-8f8fe4d61bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOLOv8 (ultralytics)\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# SSD300\n",
    "ssd_weights = SSD300_VGG16_Weights.DEFAULT\n",
    "ssd_model = ssd300_vgg16(weights=ssd_weights).to(device)\n",
    "ssd_model.eval()\n",
    "\n",
    "# Faster R-CNN\n",
    "fasterrcnn_weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "fasterrcnn_model = fasterrcnn_resnet50_fpn(weights=fasterrcnn_weights).to(device)\n",
    "fasterrcnn_model.eval()\n",
    "\n",
    "SSD_CATEGORIES = ssd_weights.meta[\"categories\"]\n",
    "FASTER_CATEGORIES = fasterrcnn_weights.meta[\"categories\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01867588-b180-468b-87d3-3ddd7813679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video_summary(\n",
    "    *,\n",
    "    model_name: str,\n",
    "    video_path,\n",
    "    total_frames: int,\n",
    "    processed_frames: int,\n",
    "    sample_every: int,\n",
    "    max_frames,\n",
    "    avg_fps: float,\n",
    "    total_person_dets: int,\n",
    "    avg_persons_per_frame: float,\n",
    "    max_persons_on_frame: int,\n",
    "    avg_confidence: float,\n",
    "    conf: float,\n",
    "    iou: float | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Возвращает словарь с метриками по видео в едином формате\n",
    "    для YOLO, SSD и Faster R-CNN.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"video_name\": Path(video_path).name,\n",
    "\n",
    "        \"frames_total\": total_frames,\n",
    "        \"frames_processed\": processed_frames,\n",
    "        \"sample_every\": sample_every,\n",
    "        \"max_frames\": max_frames,\n",
    "\n",
    "        \"avg_fps\": avg_fps,\n",
    "        \"total_person_dets\": int(total_person_dets),\n",
    "        \"avg_persons_per_frame\": avg_persons_per_frame,\n",
    "        \"max_persons_on_frame\": int(max_persons_on_frame),\n",
    "        \"avg_confidence\": avg_confidence,\n",
    "        \"conf_th\": conf,\n",
    "\n",
    "        \"iou_th\": iou, # для YOLO — число, для SSD/Faster — None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17bd008-d0d7-47f0-b9ee-056e22be5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video_yolo_simple(\n",
    "    video_path: Path,\n",
    "    model,\n",
    "    model_name: str = \"yolov8n\",\n",
    "    conf: float = CONF_TH,\n",
    "    iou: float = IOU_TH,\n",
    "    person_id: int = YOLO_PERSON_ID,\n",
    "    sample_every: int = 1,\n",
    "    max_frames: int | None = None,\n",
    "    output_path: Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Прогоняет видео через YOLOv8, опционально сохраняет размеченное видео\n",
    "    и возвращает: (output_path, summary_dict) в формате make_video_summary().\n",
    "    Метрики считаются только по людям (person_id), но на видео рисуются все классы.\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Не удалось открыть видео: {video_path}\")\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    native_fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "\n",
    "    out = None\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        fps_out = native_fps if native_fps > 0 else 25.0\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps_out, (width, height))\n",
    "\n",
    "    total_frames = 0\n",
    "    processed_frames = 0\n",
    "    total_person_dets = 0\n",
    "    max_persons_on_frame = 0\n",
    "    conf_sum = 0.0\n",
    "    conf_count = 0\n",
    "\n",
    "    frame_idx = -1\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    while True:\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        frame_idx += 1\n",
    "        total_frames += 1\n",
    "\n",
    "        if frame_idx % sample_every != 0:\n",
    "            continue\n",
    "\n",
    "        processed_frames += 1\n",
    "        if max_frames is not None and processed_frames > max_frames:\n",
    "            break\n",
    "\n",
    "        results = model(\n",
    "            frame_bgr,\n",
    "            conf=conf,\n",
    "            iou=iou,\n",
    "            verbose=False,\n",
    "        )\n",
    "        res = results[0]\n",
    "\n",
    "        if res.boxes is not None and len(res.boxes) > 0:\n",
    "            boxes_xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "            scores = res.boxes.conf.cpu().numpy()\n",
    "            classes = res.boxes.cls.cpu().numpy().astype(int)\n",
    "        else:\n",
    "            boxes_xyxy = np.empty((0, 4), dtype=np.float32)\n",
    "            scores = np.empty((0,), dtype=np.float32)\n",
    "            classes = np.empty((0,), dtype=int)\n",
    "\n",
    "        # ---- МЕТРИКИ: только по людям ----\n",
    "        person_mask = (classes == person_id)\n",
    "        person_scores = scores[person_mask]\n",
    "\n",
    "        num_persons = len(person_scores)\n",
    "        total_person_dets += num_persons\n",
    "        max_persons_on_frame = max(max_persons_on_frame, num_persons)\n",
    "\n",
    "        if num_persons > 0:\n",
    "            conf_sum += float(person_scores.sum())\n",
    "            conf_count += num_persons\n",
    "\n",
    "        if out is not None:\n",
    "            annotated = res.plot()\n",
    "            out.write(annotated)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "\n",
    "    avg_fps = processed_frames / elapsed if elapsed > 0 and processed_frames > 0 else 0.0\n",
    "    avg_persons_per_frame = (\n",
    "        total_person_dets / processed_frames if processed_frames > 0 else 0.0\n",
    "    )\n",
    "    avg_conf = conf_sum / conf_count if conf_count > 0 else 0.0\n",
    "\n",
    "    summary = make_video_summary(\n",
    "        model_name=model_name,\n",
    "        video_path=video_path,\n",
    "        total_frames=total_frames,\n",
    "        processed_frames=processed_frames,\n",
    "        sample_every=sample_every,\n",
    "        max_frames=max_frames,\n",
    "        avg_fps=avg_fps,\n",
    "        total_person_dets=total_person_dets,\n",
    "        avg_persons_per_frame=avg_persons_per_frame,\n",
    "        max_persons_on_frame=max_persons_on_frame,\n",
    "        avg_confidence=avg_conf,\n",
    "        conf=conf,\n",
    "        iou=iou,\n",
    "    )\n",
    "\n",
    "    return output_path, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d809043-2ffb-4254-979f-8b43eed32e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video_ssd300_simple(\n",
    "    video_path: Path,\n",
    "    model,\n",
    "    device,\n",
    "    model_name: str = \"ssd300_vgg16\",\n",
    "    conf: float = CONF_TH,\n",
    "    person_label: int = SSD_PERSON_LABEL,\n",
    "    sample_every: int = 1,\n",
    "    max_frames: int | None = None,\n",
    "    output_path: Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Прогоняет видео через SSD300, опционально сохраняет размеченное видео\n",
    "    и возвращает: (output_path, summary_dict) в формате make_video_summary().\n",
    "\n",
    "    Метрики считаются только по людям (person_label),\n",
    "    но на видео рисуются все классы с score >= conf\n",
    "    с подписями \"<class_name> <score>\".\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Не удалось открыть видео: {video_path}\")\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    native_fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "\n",
    "    out = None\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        fps_out = native_fps if native_fps > 0 else 25.0\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps_out, (width, height))\n",
    "\n",
    "    total_frames = 0\n",
    "    processed_frames = 0\n",
    "    total_person_dets = 0\n",
    "    max_persons_on_frame = 0\n",
    "    conf_sum = 0.0\n",
    "    conf_count = 0\n",
    "\n",
    "    frame_idx = -1\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ok, frame_bgr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            frame_idx += 1\n",
    "            total_frames += 1\n",
    "\n",
    "            if frame_idx % sample_every != 0:\n",
    "                continue\n",
    "\n",
    "            processed_frames += 1\n",
    "            if max_frames is not None and processed_frames > max_frames:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            img_tensor = F.to_tensor(frame_rgb).to(device)\n",
    "\n",
    "            outputs = model([img_tensor])[0]\n",
    "\n",
    "            boxes = outputs[\"boxes\"].detach().cpu().numpy()\n",
    "            scores = outputs[\"scores\"].detach().cpu().numpy()\n",
    "            labels = outputs[\"labels\"].detach().cpu().numpy()\n",
    "\n",
    "            # ----- МЕТРИКИ: только люди -----\n",
    "            person_mask = (labels == person_label) & (scores >= conf)\n",
    "            person_scores = scores[person_mask]\n",
    "\n",
    "            num_persons = len(person_scores)\n",
    "            total_person_dets += num_persons\n",
    "            max_persons_on_frame = max(max_persons_on_frame, num_persons)\n",
    "\n",
    "            if num_persons > 0:\n",
    "                conf_sum += float(person_scores.sum())\n",
    "                conf_count += num_persons\n",
    "\n",
    "            # ----- ВИЗУАЛИЗАЦИЯ: все классы с score >= conf -----\n",
    "            if out is not None:\n",
    "                vis_mask = (scores >= conf)\n",
    "                vis_boxes = boxes[vis_mask]\n",
    "                vis_labels = labels[vis_mask]\n",
    "                vis_scores = scores[vis_mask]\n",
    "\n",
    "                for box, lbl, scr in zip(vis_boxes, vis_labels, vis_scores):\n",
    "                    x1, y1, x2, y2 = box.astype(int)\n",
    "\n",
    "                    # один зелёный цвет\n",
    "                    cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    # подпись: <class_name> <score>\n",
    "                    class_id = int(lbl)\n",
    "                    if 0 <= class_id < len(SSD_CATEGORIES):\n",
    "                        class_name = SSD_CATEGORIES[class_id]\n",
    "                    else:\n",
    "                        class_name = str(class_id)\n",
    "\n",
    "                    text = f\"{class_name} {scr:.2f}\"\n",
    "                    cv2.putText(\n",
    "                        frame_bgr,\n",
    "                        text,\n",
    "                        (x1, max(y1 - 5, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4,\n",
    "                        (0, 255, 0),\n",
    "                        1,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "                out.write(frame_bgr)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "\n",
    "    avg_fps = processed_frames / elapsed if elapsed > 0 and processed_frames > 0 else 0.0\n",
    "    avg_persons_per_frame = (\n",
    "        total_person_dets / processed_frames if processed_frames > 0 else 0.0\n",
    "    )\n",
    "    avg_conf = conf_sum / conf_count if conf_count > 0 else 0.0\n",
    "\n",
    "    summary = make_video_summary(\n",
    "        model_name=model_name,\n",
    "        video_path=video_path,\n",
    "        total_frames=total_frames,\n",
    "        processed_frames=processed_frames,\n",
    "        sample_every=sample_every,\n",
    "        max_frames=max_frames,\n",
    "        avg_fps=avg_fps,\n",
    "        total_person_dets=total_person_dets,\n",
    "        avg_persons_per_frame=avg_persons_per_frame,\n",
    "        max_persons_on_frame=max_persons_on_frame,\n",
    "        avg_confidence=avg_conf,\n",
    "        conf=conf,\n",
    "        iou=None,\n",
    "    )\n",
    "\n",
    "    return output_path, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef7cefa-f101-4fc6-891c-68361c2e1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video_fasterrcnn_simple(\n",
    "    video_path: Path,\n",
    "    model,\n",
    "    device,\n",
    "    model_name: str = \"fasterrcnn_resnet50_fpn\",\n",
    "    conf: float = CONF_TH,\n",
    "    person_label: int = FASTER_PERSON_LABEL,\n",
    "    sample_every: int = 1,\n",
    "    max_frames: int | None = None,\n",
    "    output_path: Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Прогоняет видео через Faster R-CNN, опционально сохраняет размеченное видео\n",
    "    и возвращает: (output_path, summary_dict) в формате make_video_summary().\n",
    "\n",
    "    Метрики считаются только по людям (person_label),\n",
    "    но на видео рисуются все классы с score >= conf\n",
    "    с подписями \"<class_name> <score>\".\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Не удалось открыть видео: {video_path}\")\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    native_fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "\n",
    "    out = None\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        fps_out = native_fps if native_fps > 0 else 25.0\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps_out, (width, height))\n",
    "\n",
    "    total_frames = 0\n",
    "    processed_frames = 0\n",
    "    total_person_dets = 0\n",
    "    max_persons_on_frame = 0\n",
    "    conf_sum = 0.0\n",
    "    conf_count = 0\n",
    "\n",
    "    frame_idx = -1\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ok, frame_bgr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            frame_idx += 1\n",
    "            total_frames += 1\n",
    "\n",
    "            if frame_idx % sample_every != 0:\n",
    "                continue\n",
    "\n",
    "            processed_frames += 1\n",
    "            if max_frames is not None and processed_frames > max_frames:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            img_tensor = F.to_tensor(frame_rgb).to(device)\n",
    "\n",
    "            outputs = model([img_tensor])[0]\n",
    "\n",
    "            boxes = outputs[\"boxes\"].detach().cpu().numpy()\n",
    "            scores = outputs[\"scores\"].detach().cpu().numpy()\n",
    "            labels = outputs[\"labels\"].detach().cpu().numpy()\n",
    "\n",
    "            # ----- МЕТРИКИ: только люди -----\n",
    "            person_mask = (labels == person_label) & (scores >= conf)\n",
    "            person_scores = scores[person_mask]\n",
    "\n",
    "            num_persons = len(person_scores)\n",
    "            total_person_dets += num_persons\n",
    "            max_persons_on_frame = max(max_persons_on_frame, num_persons)\n",
    "\n",
    "            if num_persons > 0:\n",
    "                conf_sum += float(person_scores.sum())\n",
    "                conf_count += num_persons\n",
    "\n",
    "            # ----- ВИЗУАЛИЗАЦИЯ: все классы с score >= conf -----\n",
    "            if out is not None:\n",
    "                vis_mask = (scores >= conf)\n",
    "                vis_boxes = boxes[vis_mask]\n",
    "                vis_labels = labels[vis_mask]\n",
    "                vis_scores = scores[vis_mask]\n",
    "\n",
    "                for box, lbl, scr in zip(vis_boxes, vis_labels, vis_scores):\n",
    "                    x1, y1, x2, y2 = box.astype(int)\n",
    "\n",
    "                    cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    class_id = int(lbl)\n",
    "                    if 0 <= class_id < len(FASTER_CATEGORIES):\n",
    "                        class_name = FASTER_CATEGORIES[class_id]\n",
    "                    else:\n",
    "                        class_name = str(class_id)\n",
    "\n",
    "                    text = f\"{class_name} {scr:.2f}\"\n",
    "                    cv2.putText(\n",
    "                        frame_bgr,\n",
    "                        text,\n",
    "                        (x1, max(y1 - 5, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4,\n",
    "                        (0, 255, 0),\n",
    "                        1,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "                out.write(frame_bgr)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "\n",
    "    avg_fps = processed_frames / elapsed if elapsed > 0 and processed_frames > 0 else 0.0\n",
    "    avg_persons_per_frame = (\n",
    "        total_person_dets / processed_frames if processed_frames > 0 else 0.0\n",
    "    )\n",
    "    avg_conf = conf_sum / conf_count if conf_count > 0 else 0.0\n",
    "\n",
    "    summary = make_video_summary(\n",
    "        model_name=model_name,\n",
    "        video_path=video_path,\n",
    "        total_frames=total_frames,\n",
    "        processed_frames=processed_frames,\n",
    "        sample_every=sample_every,\n",
    "        max_frames=max_frames,\n",
    "        avg_fps=avg_fps,\n",
    "        total_person_dets=total_person_dets,\n",
    "        avg_persons_per_frame=avg_persons_per_frame,\n",
    "        max_persons_on_frame=max_persons_on_frame,\n",
    "        avg_confidence=avg_conf,\n",
    "        conf=conf,\n",
    "        iou=None,\n",
    "    )\n",
    "\n",
    "    return output_path, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "735b91f8-cb4a-4573-9aa9-d7a180e7ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_video(\n",
    "    video_path: Path,\n",
    "    sample_every: int = 1,\n",
    "    max_frames: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Прогоняет одно видео через YOLOv8, SSD300 и Faster R-CNN,\n",
    "    показывает прогресс и возвращает DataFrame с метриками.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    video_path = Path(video_path)\n",
    "    video_name = video_path.stem  # например \"clear_weather\"\n",
    "\n",
    "    # Пути для сохранения результатов\n",
    "    yolo_output = Path(f\"data/result/yolov8/{video_name}.mp4\")\n",
    "    ssd_output = Path(f\"data/result/ssd300/{video_name}.mp4\")\n",
    "    frcnn_output = Path(f\"data/result/fasterrcnn/{video_name}.mp4\")\n",
    "\n",
    "    # Папки создадутся внутри каждой evaluate_*_simple при необходимости,\n",
    "    # но можно и здесь:\n",
    "    yolo_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ssd_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    frcnn_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    models = [\n",
    "        (\n",
    "            \"YOLOv8\",\n",
    "            evaluate_video_yolo_simple,\n",
    "            {\n",
    "                \"video_path\": video_path,\n",
    "                \"model\": yolo_model,\n",
    "                \"model_name\": \"yolov8n\",\n",
    "                \"conf\": CONF_TH,\n",
    "                \"iou\": IOU_TH,\n",
    "                \"person_id\": YOLO_PERSON_ID,\n",
    "                \"sample_every\": sample_every,\n",
    "                \"max_frames\": max_frames,\n",
    "                \"output_path\": yolo_output,\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"SSD300\",\n",
    "            evaluate_video_ssd300_simple,\n",
    "            {\n",
    "                \"video_path\": video_path,\n",
    "                \"model\": ssd_model,\n",
    "                \"device\": device,\n",
    "                \"model_name\": \"ssd300_vgg16\",\n",
    "                \"conf\": CONF_TH,\n",
    "                \"person_label\": SSD_PERSON_LABEL,\n",
    "                \"sample_every\": sample_every,\n",
    "                \"max_frames\": max_frames,\n",
    "                \"output_path\": ssd_output,\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Faster R-CNN\",\n",
    "            evaluate_video_fasterrcnn_simple,\n",
    "            {\n",
    "                \"video_path\": video_path,\n",
    "                \"model\": fasterrcnn_model,\n",
    "                \"device\": device,\n",
    "                \"model_name\": \"fasterrcnn_resnet50_fpn\",\n",
    "                \"conf\": CONF_TH,\n",
    "                \"person_label\": FASTER_PERSON_LABEL,\n",
    "                \"sample_every\": sample_every,\n",
    "                \"max_frames\": max_frames,\n",
    "                \"output_path\": frcnn_output,\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for model_name, func, kwargs in tqdm(\n",
    "        models, desc=f\"Processing {video_path.name}\", leave=True\n",
    "    ):\n",
    "        _, summary = func(**kwargs)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return pd.DataFrame(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7c9af6a-26fd-415c-9f26-21bb61dcde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_videos(\n",
    "    video_paths,\n",
    "    sample_every: int = 1,\n",
    "    max_frames: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Прогоняет КАЖДОЕ видео через все три модели и\n",
    "    возвращает один общий DataFrame по всем (модель × видео).\n",
    "\n",
    "    video_paths:\n",
    "        - список Path/строк\n",
    "        - или dict[name -> Path/строка]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    if isinstance(video_paths, dict):\n",
    "        iterable = list(video_paths.items())\n",
    "    else:\n",
    "        iterable = [(None, vp) for vp in video_paths]\n",
    "\n",
    "    for alias, vp in tqdm(iterable, desc=\"All videos\", leave=True):\n",
    "        df_video = evaluate_models_on_video(Path(vp), sample_every, max_frames).copy()\n",
    "        if alias is not None:\n",
    "            df_video[\"video_alias\"] = alias\n",
    "        rows.append(df_video)\n",
    "\n",
    "    if rows:\n",
    "        return pd.concat(rows, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f82c920-940d-4d55-91ff-108d5b34b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All videos:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[Acessing changeable_conditions.mp4:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[Acessing changeable_conditions.mp4:  33%|███▎      | 1/3 [01:33<03:06, 93.49s/it]\n",
      "\u001b[Acessing changeable_conditions.mp4:  67%|██████▋   | 2/3 [10:30<05:54, 354.15s/it]\n",
      "Processing changeable_conditions.mp4: 100%|██████████| 3/3 [1:10:07<00:00, 1402.53s/it]\n",
      "All videos:  25%|██▌       | 1/4 [1:10:07<3:30:22, 4207.64s/it]\n",
      "\u001b[Acessing clear_weather.mp4:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[Acessing clear_weather.mp4:  33%|███▎      | 1/3 [01:41<03:23, 101.51s/it]\n",
      "\u001b[Acessing clear_weather.mp4:  67%|██████▋   | 2/3 [12:06<06:49, 409.57s/it]\n",
      "Processing clear_weather.mp4: 100%|██████████| 3/3 [1:39:22<00:00, 1987.62s/it]\n",
      "All videos:  50%|█████     | 2/4 [2:49:30<2:54:40, 5240.15s/it]\n",
      "\u001b[Acessing dark_video.mp4:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[Acessing dark_video.mp4:  33%|███▎      | 1/3 [01:48<03:37, 108.75s/it]\n",
      "Processing dark_video.mp4:  67%|██████▋   | 2/3 [10:31<05:15, 315.79s/it]\n",
      "All videos:  50%|█████     | 2/4 [3:00:02<3:00:02, 5401.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m VIDEOS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchangeable\u001b[39m\u001b[38;5;124m\"\u001b[39m: CHANGEABLE_CONDITIONS_VIDEO_PATH,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclear\u001b[39m\u001b[38;5;124m\"\u001b[39m: CLEAR_WHEATHER_VIDEO_PATH,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdark\u001b[39m\u001b[38;5;124m\"\u001b[39m: DARK_VIDEO_PATH,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoor\u001b[39m\u001b[38;5;124m\"\u001b[39m: POOR_WEATHER_VIDEO_PATH,\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# # Одно видео:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# df_one = evaluate_models_on_video(CHANGEABLE_CONDITIONS_VIDEO, sample_every=2, max_frames=300)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# display(df_one)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Все видео:\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df_all \u001b[38;5;241m=\u001b[39m evaluate_all_videos(VIDEOS)\n\u001b[0;32m     13\u001b[0m display(df_all)\n",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m, in \u001b[0;36mevaluate_all_videos\u001b[1;34m(video_paths, sample_every, max_frames)\u001b[0m\n\u001b[0;32m     19\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mNone\u001b[39;00m, vp) \u001b[38;5;28;01mfor\u001b[39;00m vp \u001b[38;5;129;01min\u001b[39;00m video_paths]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alias, vp \u001b[38;5;129;01min\u001b[39;00m tqdm(iterable, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll videos\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 22\u001b[0m     df_video \u001b[38;5;241m=\u001b[39m evaluate_models_on_video(Path(vp), sample_every, max_frames)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         df_video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_alias\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m alias\n",
      "Cell \u001b[1;32mIn[30], line 77\u001b[0m, in \u001b[0;36mevaluate_models_on_video\u001b[1;34m(video_path, sample_every, max_frames)\u001b[0m\n\u001b[0;32m     26\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     (\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLOv8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     ),\n\u001b[0;32m     72\u001b[0m ]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, func, kwargs \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     75\u001b[0m     models, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     76\u001b[0m ):\n\u001b[1;32m---> 77\u001b[0m     _, summary \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(summaries)\n",
      "Cell \u001b[1;32mIn[29], line 67\u001b[0m, in \u001b[0;36mevaluate_video_fasterrcnn_simple\u001b[1;34m(video_path, model, device, model_name, conf, person_label, sample_every, max_frames, output_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame_bgr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     65\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_tensor(frame_rgb)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model([img_tensor])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m boxes \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     70\u001b[0m scores \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:114\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    107\u001b[0m             degen_bb: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    108\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m    109\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    110\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m             )\n\u001b[1;32m--> 114\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensors)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    116\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torchvision\\models\\resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mD:\\Python\\Anaconda\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    542\u001b[0m     )\n\u001b[1;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    545\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VIDEOS = {\n",
    "    \"changeable\": CHANGEABLE_CONDITIONS_VIDEO_PATH,\n",
    "    \"clear\": CLEAR_WHEATHER_VIDEO_PATH,\n",
    "    \"dark\": DARK_VIDEO_PATH,\n",
    "    \"poor\": POOR_WEATHER_VIDEO_PATH,\n",
    "}\n",
    "# # Одно видео:\n",
    "# df_one = evaluate_models_on_video(CHANGEABLE_CONDITIONS_VIDEO, sample_every=2, max_frames=300)\n",
    "# display(df_one)\n",
    "\n",
    "# Все видео:\n",
    "df_all = evaluate_all_videos(VIDEOS)\n",
    "display(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340ae2f-d50f-4ccc-9e0a-78fb0c615ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
