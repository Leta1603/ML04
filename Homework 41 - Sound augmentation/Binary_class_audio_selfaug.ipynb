{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift"
   ],
   "metadata": {
    "id": "lG79KRKMlKmw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_fix_audio(path, target_sr=16000, target_len_sec=3.0):\n",
    "    y, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "\n",
    "    n_target = int(target_sr * target_len_sec)\n",
    "\n",
    "    if len(y) < n_target:\n",
    "        y = np.pad(y, (0, n_target - len(y)), mode='constant')\n",
    "    else:\n",
    "        y = y[:n_target]\n",
    "\n",
    "    return y"
   ],
   "metadata": {
    "id": "4UIqPiEDuqiI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# функуия для разбиения данных\n",
    "def split_dataset(root_dir, test_size=0.15, val_size=0.15, min_per_class=2):\n",
    "    root_path = Path(root_dir)\n",
    "    files = list(root_path.rglob(\"*.wav\"))\n",
    "    files = [str(p) for p in files]\n",
    "    # files = glob.glob(os.path.join(root_dir, \"**/*.wav\"), recursive=True)\n",
    "    labels = [1 if \"barbie\" in f else 0 for f in files]\n",
    "\n",
    "    n_barbie = sum(labels)\n",
    "    n_puppy = len(labels) - n_barbie\n",
    "    print(f\"Всего: {len(files)} файлов (barbie={n_barbie}, puppy={n_puppy})\")\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        files, labels, test_size=test_size + val_size, stratify=labels, random_state=42\n",
    "    )\n",
    "\n",
    "    rel_val = val_size / (test_size + val_size)\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=rel_val, stratify=y_temp, random_state=42\n",
    "    )\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n"
   ],
   "metadata": {
    "id": "cxfC43wDl-s5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_logmel(y, sr=16000, n_mels=64):\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return logmel"
   ],
   "metadata": {
    "id": "2UZebnn6oKDp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Две аугментации: слабая (почти оригинал) и сильная (для self-augmentation)\n",
    "# Идея: всегда учимся на weak, а strong \"подключаем\" через consistency loss, когда модель уверена.\n",
    "\n",
    "weak_transform = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.0003, max_amplitude=0.003, p=0.3),\n",
    "])\n",
    "\n",
    "strong_transform = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.6),\n",
    "    TimeStretch(min_rate=0.85, max_rate=1.20, p=0.6),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.6),\n",
    "    Shift(p=0.5),\n",
    "])\n"
   ],
   "metadata": {
    "id": "Dpv-OIgioVd5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Dataset для бинарной классификации аудио.\n",
    "\n",
    "    Если self_augment=True и train=True:\n",
    "      - возвращает пару (x_weak, x_strong, y)\n",
    "      - x_weak = оригинал или слабая аугментация\n",
    "      - x_strong = сильная аугментация\n",
    "\n",
    "    Иначе:\n",
    "      - возвращает (x, x, y) чтобы код обучения/валидации был единым.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files, labels, feature_type=\"logmel\", train=True, self_augment=False):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.feature_type = feature_type\n",
    "        self.train = train\n",
    "        self.self_augment = self_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _to_features(self, y):\n",
    "        if self.feature_type == \"logmel\":\n",
    "            feat = extract_logmel(y)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown feature type\")\n",
    "\n",
    "        # Нормализация по примеру\n",
    "        feat = (feat - feat.mean()) / (feat.std() + 1e-8)\n",
    "        feat = torch.tensor(feat, dtype=torch.float32).unsqueeze(0)  # [1, n_mels, time]\n",
    "        return feat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        y = load_fix_audio(path)  # 1D float32, фикс. длина/частота\n",
    "\n",
    "        # weak\n",
    "        y_w = y\n",
    "        if self.train and self.self_augment:\n",
    "            y_w = weak_transform(samples=y_w, sample_rate=16000)\n",
    "\n",
    "        # strong\n",
    "        y_s = y\n",
    "        if self.train and self.self_augment:\n",
    "            y_s = strong_transform(samples=y_s, sample_rate=16000)\n",
    "\n",
    "        x_w = self._to_features(y_w)\n",
    "        x_s = self._to_features(y_s)\n",
    "\n",
    "        return x_w, x_s, torch.tensor(label, dtype=torch.long)\n"
   ],
   "metadata": {
    "id": "JMd7qkOxpIyS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size=256):\n",
    "        super().__init__()\n",
    "\n",
    "        freq, time = input_shape\n",
    "        input_dim = freq * time\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "id": "4oj4TBlrq0EX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_dir = \"data\"\n",
    "(train_files, train_labels), (val_files, val_labels), (test_files, test_labels) = split_dataset(root_dir)\n",
    "\n",
    "feature_type = \"logmel\"\n",
    "\n",
    "# Включаем self-augmentation только на train\n",
    "train_ds = AudioDataset(train_files, train_labels, feature_type=feature_type, train=True,  self_augment=True)\n",
    "val_ds   = AudioDataset(val_files,   val_labels,   feature_type=feature_type, train=False, self_augment=False)\n",
    "test_ds  = AudioDataset(test_files,  test_labels,  feature_type=feature_type, train=False, self_augment=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=16, shuffle=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ai8cKzxFtaPV",
    "outputId": "4a8d12aa-73b8-430c-9071-015f72d3de43"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Всего: 98 файлов (barbie=50, puppy=48)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_ds)"
   ],
   "metadata": {
    "id": "MbRtVNT68sPo",
    "outputId": "d7326cbd-14e5-45ee-d139-34d8b610edcd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sample_x_w, sample_x_s, _ = train_ds[0]\n",
    "input_shape = sample_x_w.squeeze(0).shape\n",
    "input_shape\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlXY0oVstzmq",
    "outputId": "2955b9be-3431-45b4-c6e6-feec962ccd96"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([64, 94])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = MLP(input_shape=input_shape, hidden_size=256).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "id": "TPpqCjist6lu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def rampup_weight(epoch, warmup_epochs=3, rampup_epochs=5, max_weight=1.0):\n",
    "    \"\"\"Плавно наращиваем вклад consistency loss после warmup.\"\"\"\n",
    "    if epoch <= warmup_epochs:\n",
    "        return 0.0\n",
    "    t = (epoch - warmup_epochs) / max(1, rampup_epochs)\n",
    "    t = max(0.0, min(1.0, t))\n",
    "    # smoothstep\n",
    "    return max_weight * (t * t * (3 - 2 * t))\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, epoch, optimizer=None,\n",
    "              conf_thresh=0.80, warmup_epochs=3, rampup_epochs=5, lam_max=1.0):\n",
    "    \"\"\"Один проход по датасету.\n",
    "\n",
    "    Обучение:\n",
    "      loss = CE(logits_w, y) + lam(epoch) * I[conf>th] * KL(p_w || p_s)\n",
    "\n",
    "    Валидация/тест:\n",
    "      считаем только CE на weak (x_w), strong игнорируем (но датасет всё равно отдаёт x_s=x_w).\n",
    "    \"\"\"\n",
    "    train_mode = optimizer is not None\n",
    "    model.train() if train_mode else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_sup = 0.0\n",
    "    total_con = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_masked = 0\n",
    "\n",
    "    lam = rampup_weight(epoch, warmup_epochs=warmup_epochs, rampup_epochs=rampup_epochs, max_weight=lam_max)\n",
    "\n",
    "    for x_w, x_s, y in dataloader:\n",
    "        x_w = x_w.to(device)\n",
    "        x_s = x_s.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits_w = model(x_w)\n",
    "        loss_sup = criterion(logits_w, y)\n",
    "\n",
    "        loss = loss_sup\n",
    "\n",
    "        # Consistency только в train и только когда lam>0\n",
    "        if train_mode and lam > 0:\n",
    "            with torch.no_grad():\n",
    "                p_w = torch.softmax(logits_w, dim=1)\n",
    "                conf, _ = p_w.max(dim=1)\n",
    "                mask = (conf > conf_thresh).float()  # [B]\n",
    "            logits_s = model(x_s)\n",
    "            p_s = torch.softmax(logits_s, dim=1)\n",
    "\n",
    "            # KL(p_w || p_s) по батчу\n",
    "            # kl_div ожидает log-prob на входе и prob как target\n",
    "            per_sample_kl = torch.nn.functional.kl_div(\n",
    "                torch.log(p_s + 1e-8), p_w, reduction=\"none\"\n",
    "            ).sum(dim=1)\n",
    "\n",
    "            loss_con = (per_sample_kl * mask).mean()\n",
    "            loss = loss + lam * loss_con\n",
    "\n",
    "            total_con += loss_con.item() * y.size(0)\n",
    "            total_masked += int(mask.sum().item())\n",
    "\n",
    "        if train_mode:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_sup += loss_sup.item() * y.size(0)\n",
    "\n",
    "        preds = logits_w.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_sup = total_sup / total_samples\n",
    "    avg_con = total_con / max(1, total_samples)\n",
    "    avg_acc = total_correct / total_samples\n",
    "    masked_ratio = total_masked / max(1, total_samples)\n",
    "\n",
    "    return avg_loss, avg_acc, avg_sup, avg_con, masked_ratio, lam\n"
   ],
   "metadata": {
    "id": "TedKUy_zuDtQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Self-augmentation параметры\n",
    "num_epochs = 15\n",
    "conf_thresh = 0.80\n",
    "warmup_epochs = 3\n",
    "rampup_epochs = 6\n",
    "lam_max = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc, train_sup, train_con, train_masked, lam = run_epoch(\n",
    "        model, train_dl, epoch,\n",
    "        optimizer=optimizer,\n",
    "        conf_thresh=conf_thresh,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        rampup_epochs=rampup_epochs,\n",
    "        lam_max=lam_max\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, val_sup, val_con, val_masked, _ = run_epoch(\n",
    "        model, val_dl, epoch,\n",
    "        optimizer=None,\n",
    "        conf_thresh=conf_thresh,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        rampup_epochs=rampup_epochs,\n",
    "        lam_max=lam_max\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"lam={lam:.2f} | \"\n",
    "        f\"train_loss={train_loss:.4f} (sup={train_sup:.4f}, con={train_con:.4f}, masked={train_masked:.2%}) | \"\n",
    "        f\"train_acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.3f}\"\n",
    "    )\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TObUsGEIuKam",
    "outputId": "565477a0-2e81-4754-fa96-7a88bb510742"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 01 | train_loss=0.6010, train_acc=0.647 | val_loss=0.6160, val_acc=0.600\n",
      "Epoch 02 | train_loss=0.7575, train_acc=0.500 | val_loss=0.5863, val_acc=0.667\n",
      "Epoch 03 | train_loss=0.7216, train_acc=0.574 | val_loss=0.6048, val_acc=0.600\n",
      "Epoch 04 | train_loss=0.6836, train_acc=0.603 | val_loss=0.6602, val_acc=0.533\n",
      "Epoch 05 | train_loss=0.7230, train_acc=0.662 | val_loss=0.7342, val_acc=0.533\n",
      "Epoch 06 | train_loss=0.7144, train_acc=0.559 | val_loss=0.7167, val_acc=0.467\n",
      "Epoch 07 | train_loss=0.7191, train_acc=0.632 | val_loss=0.6645, val_acc=0.600\n",
      "Epoch 08 | train_loss=0.6779, train_acc=0.618 | val_loss=0.6582, val_acc=0.667\n",
      "Epoch 09 | train_loss=0.7070, train_acc=0.529 | val_loss=0.6516, val_acc=0.533\n",
      "Epoch 10 | train_loss=0.6929, train_acc=0.662 | val_loss=0.6401, val_acc=0.667\n",
      "Epoch 11 | train_loss=0.6730, train_acc=0.618 | val_loss=0.6364, val_acc=0.600\n",
      "Epoch 12 | train_loss=0.6510, train_acc=0.691 | val_loss=0.6299, val_acc=0.600\n",
      "Epoch 13 | train_loss=0.6400, train_acc=0.632 | val_loss=0.6362, val_acc=0.533\n",
      "Epoch 14 | train_loss=0.6005, train_acc=0.647 | val_loss=0.7003, val_acc=0.533\n",
      "Epoch 15 | train_loss=0.5699, train_acc=0.735 | val_loss=0.7384, val_acc=0.533\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss, test_acc, test_sup, test_con, test_masked, _ = run_epoch(\n",
    "    model, test_dl, epoch=num_epochs, optimizer=None\n",
    ")\n",
    "print(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDx-fJJB05tE",
    "outputId": "f25ecbca-a971-4b58-ce6c-b8e2ff537268"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test: loss=0.6481, acc=0.467\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_w, x_s, y in dataloader:\n",
    "            x_w = x_w.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x_w)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)\n"
   ],
   "metadata": {
    "id": "mQmytJpt1guX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Распределение в тестовой выборке:\")\n",
    "print(f\"  True labels:  unique={np.unique(test_labels)}, counts={np.bincount(test_labels)}\")\n",
    "print(f\"  Predictions:  unique={np.unique(test_preds)}, counts={np.bincount(test_preds)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCFqnWAM20Ex",
    "outputId": "226581d9-7151-47ad-b5b9-cf88b2f7cae2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Распределение в тестовой выборке:\n",
      "  True labels:  unique=[0 1], counts=[8 7]\n",
      "  Predictions:  unique=[0 1], counts=[6 9]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class_names = ['puppy', 'barbie']\n",
    "\n",
    "print(classification_report(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    labels=[0, 1],\n",
    "    target_names=class_names,\n",
    "    zero_division=0\n",
    "))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nq_8Y5BN2-hf",
    "outputId": "79b65a19-7e04-4be8-8699-f65731325d81"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       puppy       0.00      0.00      0.00         0\n",
      "      barbie       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       0.50      0.50      0.50        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ]
  }
 ]
}